{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from time import perf_counter as timer"
      ],
      "metadata": {
        "id": "KdjIZWPYQSoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0wa6ejNPxxJ"
      },
      "outputs": [],
      "source": [
        "class NewsArticleProcessor:\n",
        "    def __init__(self, json_path, min_token_length=30, n_resources_to_return=10):\n",
        "        self.json_path = json_path\n",
        "        self.embedding_model_name = \"all-mpnet-base-v2\"\n",
        "        self.min_token_length = min_token_length\n",
        "        self.n_resources_to_return = n_resources_to_return\n",
        "        self.embedding_model = SentenceTransformer(model_name_or_path=self.embedding_model_name, device=\"cpu\")\n",
        "        self.df = None\n",
        "        self.articles_and_chunks = []\n",
        "        self.save_path = os.path.join(os.getcwd(), 'articles_and_embeddings_df.csv')\n",
        "\n",
        "    def load_json(self):\n",
        "        with open(self.json_path, 'r') as json_file:\n",
        "            data = json.load(json_file)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text: str) -> str:\n",
        "        return text.replace('\\n', '')\n",
        "\n",
        "    def preprocess_data(self, data):\n",
        "        for article in data:\n",
        "            article['articleBody'] = self.clean_text(article['articleBody'])\n",
        "        self.df = pd.DataFrame(data)\n",
        "        self.df['joined_article'] = self.df['title'] + ' ' + self.df['articleBody']\n",
        "\n",
        "    def process_articles(self):\n",
        "        for _, row in tqdm(self.df.iterrows(), total=self.df.shape[0]):\n",
        "            article = row['joined_article']\n",
        "            chunk_dict = {}\n",
        "\n",
        "            article = article.replace(\"  \", \" \").strip()\n",
        "            article = re.sub(r'\\.([A-Z])', r'. \\1', article)\n",
        "\n",
        "            chunk_dict[\"joined_article\"] = article\n",
        "            chunk_dict[\"article_char_count\"] = len(article)\n",
        "            chunk_dict[\"article_word_count\"] = len(article.split())\n",
        "            chunk_dict[\"article_token_count\"] = len(article) / 4\n",
        "            self.articles_and_chunks.append(chunk_dict)\n",
        "\n",
        "        self.df = pd.DataFrame(self.articles_and_chunks)\n",
        "\n",
        "    def filter_and_embed_articles(self):\n",
        "        articles_over_min_token_len = self.df[self.df[\"article_token_count\"] > self.min_token_length].to_dict(orient=\"records\")\n",
        "\n",
        "        for item in tqdm(articles_over_min_token_len):\n",
        "            item[\"embedding\"] = self.embedding_model.encode(item[\"joined_article\"])\n",
        "\n",
        "        self.df = pd.DataFrame(articles_over_min_token_len)\n",
        "\n",
        "    def save_to_csv(self):\n",
        "        self.df.to_csv(self.save_path, index=False)\n",
        "\n",
        "    def load_from_csv(self):\n",
        "        self.df = pd.read_csv(self.save_path)\n",
        "        return self.df\n",
        "\n",
        "    def process(self, load_csv=False):\n",
        "        data = self.load_json()\n",
        "        self.preprocess_data(data)\n",
        "        self.process_articles()\n",
        "        self.filter_and_embed_articles()\n",
        "        self.save_to_csv()\n",
        "\n",
        "        if load_csv:\n",
        "            return self.load_from_csv()\n",
        "\n",
        "    def prepare_embeddings(self):\n",
        "        self.df[\"embedding\"] = self.df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
        "        self.articles = self.df.to_dict(orient=\"records\")\n",
        "        self.embeddings = torch.tensor(np.array(self.df[\"embedding\"].tolist()), dtype=torch.float32).to(\"cpu\")\n",
        "\n",
        "    def retrieve_relevant_resources(self, query: str, print_time: bool=True):\n",
        "        query_embedding = self.embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "        start_time = timer()\n",
        "        dot_scores = util.dot_score(query_embedding, self.embeddings)[0]\n",
        "        end_time = timer()\n",
        "\n",
        "        if print_time:\n",
        "            print(f\"[INFO] Time taken to get scores on {len(self.embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n",
        "        scores, indices = torch.topk(input=dot_scores, k=self.n_resources_to_return)\n",
        "        relevant_articles = [self.articles[index][\"joined_article\"] for index in indices]\n",
        "\n",
        "        return relevant_articles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuestionAnswering:\n",
        "    def __init__(self, model_name=\"t5-base\"):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    @staticmethod\n",
        "    def array_to_string_with_newlines(arr):\n",
        "        return '\\n\\n\\n\\n'.join(arr)\n",
        "\n",
        "    def generate_answer(self, question, context):\n",
        "        input_text = f\"Answer the question: {question} using the context: {context}\"\n",
        "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "        outputs = self.model.generate(inputs, max_length=1000, num_beams=10, early_stopping=True, repetition_penalty=2.0, length_penalty=1.5, temperature=0.7)\n",
        "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return answer"
      ],
      "metadata": {
        "id": "2es1MuwXXkiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_articles(json_path):\n",
        "    processor = NewsArticleProcessor(json_path=json_path)\n",
        "    processor.process(load_csv=True)\n",
        "    processor.prepare_embeddings()\n",
        "    return processor\n",
        "\n",
        "def get_answer(processor, question):\n",
        "    relevant_articles = processor.retrieve_relevant_resources(query=question)\n",
        "\n",
        "    qa = QuestionAnswering()\n",
        "    context_text = qa.array_to_string_with_newlines(relevant_articles)\n",
        "    answer = qa.generate_answer(question=question, context=context_text)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "u2lMxuyNYXlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_path = 'news.json'\n",
        "processor = preprocess_articles(json_path)"
      ],
      "metadata": {
        "id": "7wwoDUeNQi4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What happened at the Al-Shifa Hospital?\"\n",
        "answer = get_answer(processor, question)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "id": "RN2aC-xHYweN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}